{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nuevo_ modelo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LuisFerRosas/ia3/blob/nuevo/nuevo__modelo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6Z1r58emPvZ"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch import Tensor \n",
        "import math\n",
        "import torch.nn.functional as F  "
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dXDdelqn6jP"
      },
      "source": [
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens: Tensor):\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self,\n",
        "                emb_size: int,\n",
        "                dropout: float,\n",
        "                maxlen: int = 6000):\n",
        "      super(PositionalEncoding, self).__init__()\n",
        "      den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
        "      pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "      pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "      pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "      pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "      pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "\n",
        "      self.dropout = nn.Dropout(dropout)\n",
        "      self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "  def forward(self, token_embedding: Tensor):\n",
        "      return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pM1DWqnZm7K3"
      },
      "source": [
        "class TmusicTrasforms(nn.Module):\n",
        "  def __init__(self,tgt_vocabulario):\n",
        "    super().__init__() \n",
        "    \n",
        "    ################ TRANSFORMER BLOCK #############################\n",
        "    # maxpool the input feature map/tensor to the transformer \n",
        "    # a rectangular kernel worked better here for the rectangular input spectrogram feature map/tensor\n",
        "    self.transformer_maxpool = nn.MaxPool2d(kernel_size=[1,4], stride=[1,4])\n",
        "    \n",
        "    \n",
        "    self.tgt_tok_emb = TokenEmbedding(tgt_vocabulario, 512)\n",
        "    self.positional_encoding = PositionalEncoding(512, dropout=0.3)\n",
        "    \n",
        "    transformer_layer = nn.TransformerEncoderLayer(\n",
        "        d_model=40, # input feature (frequency) dim after maxpooling 40*282 -> 40*70 (MFC*time)\n",
        "        nhead=4, # 4 self-attention layers in each multi-head self-attention layer in each encoder block\n",
        "        dim_feedforward=512, # 2 linear layers in each encoder block's feedforward network: dim 40-->512--->40\n",
        "        dropout=0.4, \n",
        "        activation='relu' # ReLU: avoid saturation/tame gradient/reduce compute time\n",
        "    )\n",
        "    \n",
        "    self.transformer_encoder = nn.TransformerEncoder(transformer_layer, num_layers=4)\n",
        "    \n",
        "    transformer_decoder_layer=nn.TransformerDecoderLayer(\n",
        "        d_model=512,\n",
        "        nhead=4,\n",
        "        dim_feedforward=512,\n",
        "        dropout=0.4,\n",
        "        activation='relu'\n",
        "    )\n",
        "    self.transformer_decoder=nn.TransformerDecoder(transformer_decoder_layer,num_layers=6)\n",
        "\n",
        "    ############### 1ST PARALLEL 2D CONVOLUTION BLOCK ############\n",
        "    # 3 sequential conv2D layers: (1,40,282) --> (16, 20, 141) -> (32, 5, 35) -> (64, 1, 8)\n",
        "    self.conv2Dblock1 = nn.Sequential(\n",
        "        \n",
        "        # 1st 2D convolution layer\n",
        "        nn.Conv2d(\n",
        "            in_channels=1, # input volume depth == input channel dim == 1\n",
        "            out_channels=8, # expand output feature map volume's depth to 16\n",
        "            kernel_size=3, # typical 3*3 stride 1 kernel\n",
        "            stride=1,\n",
        "            padding=1\n",
        "                  ),\n",
        "        nn.BatchNorm2d(8), # batch normalize the output feature map before activation\n",
        "        nn.ReLU(), # feature map --> activation map\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2), #typical maxpool kernel size\n",
        "        nn.Dropout(p=0.3), #randomly zero 30% of 1st layer's output feature map in training\n",
        "        \n",
        "        # 2nd 2D convolution layer identical to last except output dim, maxpool kernel\n",
        "        nn.Conv2d(\n",
        "            in_channels=8, \n",
        "            out_channels=16, # expand output feature map volume's depth to 32\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            padding=1\n",
        "                  ),\n",
        "        nn.BatchNorm2d(16),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=5, stride=5), # increase maxpool kernel for subsequent filters\n",
        "        nn.Dropout(p=0.3), \n",
        "\n",
        "        # 2nd 2D convolution layer identical to last except output dim, maxpool kernel\n",
        "        nn.Conv2d(\n",
        "            in_channels=16, \n",
        "            out_channels=32, # expand output feature map volume's depth to 32\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            padding=1\n",
        "                  ),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2), # increase maxpool kernel for subsequent filters\n",
        "        nn.Dropout(p=0.3), \n",
        "        \n",
        "        # 3rd 2D convolution layer identical to last except output dim\n",
        "        nn.Conv2d(\n",
        "            in_channels=32,\n",
        "            out_channels=64, # expand output feature map volume's depth to 64\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            padding=1\n",
        "                  ),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        nn.Dropout(p=0.3),\n",
        "    )\n",
        "    ############### 2ND PARALLEL 2D CONVOLUTION BLOCK ############\n",
        "    # 3 sequential conv2D layers: (1,40,282) --> (16, 20, 141) -> (32, 5, 35) -> (64, 1, 8)\n",
        "    self.conv2Dblock2 = nn.Sequential(\n",
        "      # 1st 2D convolution layer\n",
        "        nn.Conv2d(\n",
        "            in_channels=1, # input volume depth == input channel dim == 1\n",
        "            out_channels=8, # expand output feature map volume's depth to 16\n",
        "            kernel_size=3, # typical 3*3 stride 1 kernel\n",
        "            stride=1,\n",
        "            padding=1\n",
        "                  ),\n",
        "        nn.BatchNorm2d(8), # batch normalize the output feature map before activation\n",
        "        nn.ReLU(), # feature map --> activation map\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2), #typical maxpool kernel size\n",
        "        nn.Dropout(p=0.3), #randomly zero 30% of 1st layer's output feature map in training\n",
        "        \n",
        "        # 2nd 2D convolution layer identical to last except output dim, maxpool kernel\n",
        "        nn.Conv2d(\n",
        "            in_channels=8, \n",
        "            out_channels=16, # expand output feature map volume's depth to 32\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            padding=1\n",
        "                  ),\n",
        "        nn.BatchNorm2d(16),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=5, stride=5), # increase maxpool kernel for subsequent filters\n",
        "        nn.Dropout(p=0.3), \n",
        "\n",
        "        # 2nd 2D convolution layer identical to last except output dim, maxpool kernel\n",
        "        nn.Conv2d(\n",
        "            in_channels=16, \n",
        "            out_channels=32, # expand output feature map volume's depth to 32\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            padding=1\n",
        "                  ),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2), # increase maxpool kernel for subsequent filters\n",
        "        nn.Dropout(p=0.3), \n",
        "        \n",
        "        # 3rd 2D convolution layer identical to last except output dim\n",
        "        nn.Conv2d(\n",
        "            in_channels=32,\n",
        "            out_channels=64, # expand output feature map volume's depth to 64\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            padding=1\n",
        "                  ),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        nn.Dropout(p=0.3),\n",
        "    \n",
        "    )\n",
        "\n",
        "    ################# FINAL LINEAR BLOCK ####################\n",
        "    # Linear softmax layer to take final concatenated embedding tensor \n",
        "    #    from parallel 2D convolutional and transformer blocks, output 8 logits \n",
        "    # Each full convolution block outputs (64*1*8) embedding flattened to dim 512 1D array \n",
        "    # Full transformer block outputs 40*70 feature map, which we time-avg to dim 40 1D array\n",
        "    # 512*2+40 == 1064 input features --> 8 output emotions \n",
        "    \n",
        "    self.fc2_linear = nn.Linear(512,tgt_vocabulario)\n",
        "    \n",
        "    ### Softmax layer for the 8 output logits from final FC linear layer \n",
        "    self.softmax_out = nn.Softmax(dim=1) # dim==1 is the freq embedding\n",
        "    \n",
        "    # define one complete parallel fwd pass of input feature tensor thru 2*conv+1*transformer blocks\n",
        "  def forward(self,x,partitura_tok):\n",
        "    \n",
        "    ############ 1st parallel Conv2D block: 4 Convolutional layers ############################\n",
        "    # create final feature embedding from 1st convolutional layer \n",
        "    # input features pased through 4 sequential 2D convolutional layers\n",
        "    conv2d_embedding1 = self.conv2Dblock1(x) # x == N/batch * channel * freq * time\n",
        "    # print(\"conv2d_embedding1 : \"+str(conv2d_embedding1.shape))\n",
        "    # flatten final 64*1*8 feature map from convolutional layers to length 512 1D array \n",
        "    # skip the 1st (N/batch) dimension when flattening\n",
        "    conv2d_embedding1 = torch.flatten(conv2d_embedding1, start_dim=1) \n",
        "    # print(\"conv2d_embedding1 flatten : \"+str(conv2d_embedding1.shape))\n",
        "    ############ 2nd parallel Conv2D block: 4 Convolutional layers #############################\n",
        "    # create final feature embedding from 2nd convolutional layer \n",
        "    # input features pased through 4 sequential 2D convolutional layers\n",
        "    conv2d_embedding2 = self.conv2Dblock2(x) # x == N/batch * channel * freq * time\n",
        "    # print(\"conv2d_embedding2 : \"+str(conv2d_embedding2.shape))\n",
        "    # flatten final 64*1*8 feature map from convolutional layers to length 512 1D array \n",
        "    # skip the 1st (N/batch) dimension when flattening\n",
        "    conv2d_embedding2 = torch.flatten(conv2d_embedding2, start_dim=1) \n",
        "    # print(\"conv2d_embedding2 flatten : \"+str(conv2d_embedding2.shape))\n",
        "      \n",
        "    ########## 4-encoder-layer Transformer block w/ 40-->512-->40 feedfwd network ##############\n",
        "    # maxpool input feature map: 1*40*282 w/ 1*4 kernel --> 1*40*70 \n",
        "    x_maxpool = self.transformer_maxpool(x)\n",
        "    # print(\"x_maxpool : \"+str(x_maxpool.shape))\n",
        "    # remove channel dim: 1*40*70 --> 40*70\n",
        "    x_maxpool_reduced = torch.squeeze(x_maxpool,1)\n",
        "    # print(\"x_maxpool_reduced : \"+str(x_maxpool_reduced.shape))\n",
        "    # convert maxpooled feature map format: batch * freq * time ---> time * batch * freq format\n",
        "    # because transformer encoder layer requires tensor in format: time * batch * embedding (freq)\n",
        "    x = x_maxpool_reduced.permute(2,0,1) \n",
        "    # print(\"x------>entrada transformer : \"+str(x.shape))\n",
        "    # finally, pass reduced input feature map x into transformer encoder layers\n",
        "    transformer_output = self.transformer_encoder(x)\n",
        "    # print(\"salida transformer : \"+str(transformer_output.shape))\n",
        "    \n",
        "    # create final feature emedding from transformer layer by taking mean in the time dimension (now the 0th dim)\n",
        "    # transformer outputs 2x40 (MFCC embedding*time) feature map, take mean of columns i.e. take time average\n",
        "    transformer_embedding = torch.mean(transformer_output, dim=0) # dim 40x70 --> 40\n",
        "    # print(\"transformer_embedding media : \"+str(transformer_embedding.shape))\n",
        "  \n",
        "    complete_embedding = torch.cat([conv2d_embedding1, conv2d_embedding2,transformer_embedding], dim=1)  \n",
        "    # print(\"complete_embedding  : \"+str(complete_embedding.shape))\n",
        "    cambDim = complete_embedding.unsqueeze(2)\n",
        "    # print(\"cambDim  : \"+str(cambDim.shape))\n",
        "\n",
        "    expandido=cambDim.expand([2,5544,512])\n",
        "    # print(\"expandido  : \"+str(expandido.shape))\n",
        "\n",
        "    memory=expandido.transpose(1,0)\n",
        "    # print(\"memory  : \"+str(memory.shape))\n",
        "      \n",
        "    partitura_tok=partitura_tok.transpose(1,0)\n",
        "    # print(\"partitura_tok  : \"+str(partitura_tok.shape))\n",
        "    partitura_tok= self.tgt_tok_emb(partitura_tok)\n",
        "    # print(\"partitura_tok2  : \"+str(partitura_tok.shape))\n",
        "    tgt_emb = self.positional_encoding(partitura_tok)\n",
        "    # print(\"tgt_emb_positional  : \"+str(tgt_emb.shape))\n",
        "\n",
        "    mask_tgt=generate_square_subsequent_mask(tgt_emb.shape[0])\n",
        "    ouput_decoder=self.transformer_decoder(tgt_emb,memory,mask_tgt)\n",
        "    # print(\"ouput_decoder  : \"+str(ouput_decoder.shape))\n",
        "    linear_decoder=self.fc2_linear(ouput_decoder)\n",
        "    # print(\"linear_decoder  : \"+str(linear_decoder.shape))\n",
        "   \n",
        "   \n",
        "    ######### Final Softmax layer: use logits from FC linear, get softmax for prediction ######\n",
        "    output_softmax = self.softmax_out(linear_decoder)\n",
        "    # print(\"output_softmax  : \"+str(output_softmax.shape))\n",
        "    # need output logits to compute cross entropy loss, need softmax probabilities to predict class\n",
        "    return  output_softmax \n",
        "  \n",
        "  \n",
        "\n",
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMz699GqFy5J"
      },
      "source": [
        "from sklearn.metrics import precision_score\n",
        "def make_train_step(model, criterion, optimizer):\n",
        "    \n",
        "  def train_step(espectro,partituraT,lenPartituras):\n",
        "          \n",
        "    output_softmax = model(espectro,partituraT)\n",
        "    salida =output_softmax.transpose(0,1)\n",
        "   \n",
        "   \n",
        "    sumAcuraccy=0\n",
        "    for i in range(salida.shape[0]):\n",
        "      predictions=torch.argmax(salida[i],dim=1)\n",
        "      acuraccy=precision_score(predictions,partituraT[i],average='micro')\n",
        "      \n",
        "      # print(\"acuraccy :: \"+str(acuraccy))\n",
        "      \n",
        "      sumAcuraccy+=acuraccy\n",
        "\n",
        "    sumAcuraccy=sumAcuraccy/salida.shape[0]\n",
        "    \n",
        "    output_lengths = torch.full((output_softmax.shape[1],), output_softmax.shape[0], dtype=torch.long)\n",
        "    \n",
        "    loss = criterion(output_softmax,partituraT,output_lengths,lenPartituras) \n",
        "    \n",
        "    # compute gradients for the optimizer to use \n",
        "    # loss.backward()\n",
        "    \n",
        "    # update network parameters based on gradient stored (by calling loss.backward())\n",
        "    # optimizer.step()\n",
        "    \n",
        "    # zero out gradients for next pass\n",
        "    # pytorch accumulates gradients from backwards passes (convenient for RNNs)\n",
        "    # optimizer.zero_grad() \n",
        "    \n",
        "    return loss.item(),sumAcuraccy\n",
        "  return train_step"
      ],
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCyh-upyeh7G"
      },
      "source": [
        "def make_validate_fnc(model,criterion):\n",
        "  def validate(espectro,partituraT,lenPartituras):\n",
        "      \n",
        "    # don't want to update any network parameters on validation passes: don't need gradient\n",
        "    # wrap in torch.no_grad to save memory and compute in validation phase: \n",
        "    with torch.no_grad(): \n",
        "        \n",
        "      # set model to validation phase i.e. turn off dropout and batchnorm layers \n",
        "      model.eval()\n",
        "\n",
        "      # get the model's predictions on the validation set\n",
        "      output_softmax = model(espectro,partituraT)\n",
        "      salida =output_softmax.transpose(0,1)\n",
        "      sumAcuraccy=0\n",
        "      for i in range(salida.shape[0]):\n",
        "        predictions=torch.argmax(salida[i],dim=1)\n",
        "        acuraccy=precision_score(predictions,partituraT[i],average='micro')\n",
        "        # print(\"acuraccy :: \"+str(acuraccy))      \n",
        "        sumAcuraccy+=acuraccy\n",
        "\n",
        "    sumAcuraccy=sumAcuraccy/salida.shape[0]\n",
        "    \n",
        "    output_lengths = torch.full((output_softmax.shape[1],), output_softmax.shape[0], dtype=torch.long)\n",
        "    \n",
        "    loss = criterion(output_softmax,partituraT,output_lengths,lenPartituras) \n",
        "        \n",
        "    return loss.item(),sumAcuraccy\n",
        "  return validate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2rD88zEgdQK"
      },
      "source": [
        "def make_save_checkpoint(): \n",
        "    def save_checkpoint(optimizer, model, epoch, filename):\n",
        "        checkpoint_dict = {\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "            'model': model.state_dict(),\n",
        "            'epoch': epoch\n",
        "        }\n",
        "        torch.save(checkpoint_dict, filename)\n",
        "    return save_checkpoint\n",
        "\n",
        "def load_checkpoint(optimizer, model, filename):\n",
        "    checkpoint_dict = torch.load(filename)\n",
        "    epoch = checkpoint_dict['epoch']\n",
        "    model.load_state_dict(checkpoint_dict['model'])\n",
        "    if optimizer is not None:\n",
        "        optimizer.load_state_dict(checkpoint_dict['optimizer'])\n",
        "    return epoch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwRVQ_RHh_ox"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcJR6Yq31a1w",
        "outputId": "6ffd22b6-a5a7-472c-b0fa-7baeafeef6f0"
      },
      "source": [
        "import torch\n",
        "inputt = torch.randn(2,1, 40,1723, requires_grad=True)\n",
        "partitura = torch.randint( 0,132, (2,100))\n",
        "pato = torch.randn(2,5544, requires_grad=True)\n",
        "lenParitura = torch.randint( 30,50, (2,))\n",
        "prueba = torch.randn(100,2,512, requires_grad=True)\n",
        "print(inputt.shape)\n",
        "print(partitura.shape)\n",
        "print(lenParitura)\n",
        "print(prueba.shape[0])"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 1, 40, 1723])\n",
            "torch.Size([2, 100])\n",
            "tensor([43, 35])\n",
            "100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m43CD3mgLEhD"
      },
      "source": [
        "model=TmusicTrasforms(tgt_vocabulario=233)\n",
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model=model.to(DEVICE)\n",
        "for p in model.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDcs6SUZG1cg"
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "criterion = nn.CTCLoss()"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Medw2EI8GmDl"
      },
      "source": [
        ""
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNx6zXYmuQCw",
        "outputId": "3c8892f0-d97e-4476-9539-05c125a463f8"
      },
      "source": [
        "train_step=make_train_step(model,criterion,optimizer)\n",
        "loss=train_step(inputt,partitura,lenParitura)\n",
        "loss"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv2d_embedding1 : torch.Size([2, 64, 1, 43])\n",
            "conv2d_embedding1 flatten : torch.Size([2, 2752])\n",
            "conv2d_embedding2 : torch.Size([2, 64, 1, 43])\n",
            "conv2d_embedding2 flatten : torch.Size([2, 2752])\n",
            "x_maxpool : torch.Size([2, 1, 40, 430])\n",
            "x_maxpool_reduced : torch.Size([2, 40, 430])\n",
            "x------>entrada transformer : torch.Size([430, 2, 40])\n",
            "salida transformer : torch.Size([430, 2, 40])\n",
            "transformer_embedding media : torch.Size([2, 40])\n",
            "complete_embedding  : torch.Size([2, 5544])\n",
            "cambDim  : torch.Size([2, 5544, 1])\n",
            "expandido  : torch.Size([2, 5544, 512])\n",
            "memory  : torch.Size([5544, 2, 512])\n",
            "partitura_tok  : torch.Size([100, 2])\n",
            "partitura_tok2  : torch.Size([100, 2, 512])\n",
            "tgt_emb_positional  : torch.Size([100, 2, 512])\n",
            "ouput_decoder  : torch.Size([100, 2, 512])\n",
            "linear_decoder  : torch.Size([100, 2, 233])\n",
            "output_softmax  : torch.Size([100, 2, 233])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-3.7511916160583496, 0.005)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Omrl7QgsLsrC",
        "outputId": "ff9ac7f3-fecb-47dc-9fb1-deb8bf235af2"
      },
      "source": [
        "model.train()\n",
        "ouput=model(inputt,partitura)\n",
        "ouput"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv2d_embedding1 : torch.Size([2, 64, 1, 43])\n",
            "conv2d_embedding1 flatten : torch.Size([2, 2752])\n",
            "conv2d_embedding2 : torch.Size([2, 64, 1, 43])\n",
            "conv2d_embedding2 flatten : torch.Size([2, 2752])\n",
            "x_maxpool : torch.Size([2, 1, 40, 430])\n",
            "x_maxpool_reduced : torch.Size([2, 40, 430])\n",
            "x------>entrada transformer : torch.Size([430, 2, 40])\n",
            "salida transformer : torch.Size([430, 2, 40])\n",
            "transformer_embedding media : torch.Size([2, 40])\n",
            "complete_embedding  : torch.Size([2, 5544])\n",
            "cambDim  : torch.Size([2, 5544, 1])\n",
            "expandido  : torch.Size([2, 5544, 512])\n",
            "memory  : torch.Size([5544, 2, 512])\n",
            "partitura_tok  : torch.Size([100, 2])\n",
            "partitura_tok2  : torch.Size([100, 2, 512])\n",
            "tgt_emb_positional  : torch.Size([100, 2, 512])\n",
            "ouput_decoder  : torch.Size([100, 2, 512])\n",
            "linear_decoder  : torch.Size([100, 2, 233])\n",
            "output_softmax  : torch.Size([100, 2, 233])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.9762, 0.8009, 0.5210,  ..., 0.3210, 0.2620, 0.7164],\n",
              "         [0.0238, 0.1991, 0.4790,  ..., 0.6790, 0.7380, 0.2836]],\n",
              "\n",
              "        [[0.4998, 0.6676, 0.0292,  ..., 0.8694, 0.4351, 0.4776],\n",
              "         [0.5002, 0.3324, 0.9708,  ..., 0.1306, 0.5649, 0.5224]],\n",
              "\n",
              "        [[0.0164, 0.4358, 0.6146,  ..., 0.8432, 0.2852, 0.3257],\n",
              "         [0.9836, 0.5642, 0.3854,  ..., 0.1568, 0.7148, 0.6743]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0.6765, 0.2190, 0.9363,  ..., 0.9026, 0.3726, 0.5867],\n",
              "         [0.3235, 0.7810, 0.0637,  ..., 0.0974, 0.6274, 0.4133]],\n",
              "\n",
              "        [[0.5789, 0.6902, 0.6287,  ..., 0.5929, 0.6572, 0.3143],\n",
              "         [0.4211, 0.3098, 0.3713,  ..., 0.4071, 0.3428, 0.6857]],\n",
              "\n",
              "        [[0.4938, 0.7316, 0.7019,  ..., 0.8561, 0.6548, 0.8672],\n",
              "         [0.5062, 0.2684, 0.2981,  ..., 0.1439, 0.3452, 0.1328]]],\n",
              "       grad_fn=<SoftmaxBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stsn0YOiMr07"
      },
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "# need device to instantiate model\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# instantiate model for 8 emotions and move to GPU \n",
        "model = TmusicTrasforms(8).to(device)\n",
        "\n",
        "# include input feature map dims in call to summary()\n",
        "summary(model, input_size=[(1,40,1723),(1,100)])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nG1FU1ZlJVD"
      },
      "source": [
        "alto=torch.max(pato)\n",
        "alto"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTkaE-eneiis"
      },
      "source": [
        "emnb =nn.Embedding(133,512)\n",
        "\n",
        "salida=emnb(partitura.long())\n",
        "salida.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzdmmtabsXku"
      },
      "source": [
        "pato[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gopw3B0Ss8z5"
      },
      "source": [
        "pato.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaGll8Xmjxcp"
      },
      "source": [
        "x = pato.unsqueeze(2)\n",
        "print(x.size())\n",
        "\n",
        "x=x.expand([2,5544,512])\n",
        "print(x.size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgO5vFlrsgdz"
      },
      "source": [
        "x[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E47vyryr5z-v",
        "outputId": "12dd3812-976b-4a48-b002-7ade99bc4763"
      },
      "source": [
        "ys = torch.ones(1, 1).fill_(1).type(torch.long)\n",
        "ys.shape"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fh95UHmSPrwl"
      },
      "source": [
        "ys = torch.cat([ys,torch.ones(1, 1).type_as().fill_(45)], dim=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfL1qVyHJnM3",
        "outputId": "87731e93-9ee2-4abe-e769-2dcf53f78ef9"
      },
      "source": [
        "log_probs = torch.randn(100, 2, 233).log_softmax(2).detach().requires_grad_()\n",
        "print(log_probs.shape)#salida del modelo\n",
        "targets = torch.randint(0, 233, (2, 100), dtype=torch.long)\n",
        "print(targets.shape)#tokens del las partituras\n",
        "input_lengths = torch.full((2,), 50, dtype=torch.long)\n",
        "print(input_lengths)#salida de 100 caracteristicas salida decoder\n",
        "target_lengths = torch.randint(10,30,(2,), dtype=torch.long)\n",
        "print(target_lengths)#tamanio de las partituras"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 2, 233])\n",
            "torch.Size([2, 100])\n",
            "tensor([50, 50])\n",
            "tensor([18, 16])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMP6KnWfp4mG",
        "outputId": "5b199f94-d770-44a8-ed70-a78953dbc09d"
      },
      "source": [
        "log_probs.shape[1]"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jY14yO5pKMea",
        "outputId": "c129fc48-7fa2-4c97-98b2-e783e47999e3"
      },
      "source": [
        "import torch.nn.functional as F  \n",
        "criterion =nn.CTCLoss()\n",
        "loss=criterion(log_probs, targets, input_lengths, target_lengths)\n",
        "loss"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(13.9402, grad_fn=<MeanBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    }
  ]
}